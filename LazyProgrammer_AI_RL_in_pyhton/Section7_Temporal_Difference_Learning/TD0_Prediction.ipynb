{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de8527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464f20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment description\n",
    "rewards={(0,3):1,(1,3):-1}\n",
    "\n",
    "actions={\n",
    "    (2,0):['U','R'],\n",
    "    (1,0):['U','D'],\n",
    "    (0,0):['R','D'],\n",
    "    (2,1):['R','L'],\n",
    "    (0,1):['R','L'],\n",
    "    (2,2):['U','R','L'],\n",
    "    (1,2):['U','D','R'],\n",
    "    (0,2):['R','L','D'],\n",
    "    (2,3):['L'],\n",
    "}\n",
    "\n",
    "probs = {\n",
    "    ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "    ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "    ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "    ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "    ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "    ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "    ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "    ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "    ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "    ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "    ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "    ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "    ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "    ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "  }\n",
    "\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U'\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47134f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld  class\n",
    "class WindyGridWorld():\n",
    "    \n",
    "    def __init__(self, rows, columns, start_position):\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        #self.all_states = [(i,j) for i in range(rows) for j in range(columns)]\n",
    "        self.i = start_position[0]\n",
    "        self.j = start_position[1]\n",
    "        \n",
    "    def set_rewards_actions(self, rewards, actions, probs):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.probs = probs\n",
    "        self.all_states = set(self.actions.keys()) | set(self.rewards.keys())\n",
    "        #print (self.all_states)\n",
    "    \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.i,self.j\n",
    "    \n",
    "    def reset(self):\n",
    "        self.i = 2\n",
    "        self.j = 0\n",
    "        return self.current_state()\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action in self.actions[(self.i,self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j -= 1\n",
    "            elif action == 'L':\n",
    "                self.j += 1\n",
    "            else:\n",
    "                self.i -= 1\n",
    "        # should never happen\n",
    "        assert (self.current_state() in self.all_states)\n",
    " \n",
    "    def move(self, action):\n",
    "        cur_state = (self.i, self.j)\n",
    "        a = action\n",
    "        next_action_prob = self.probs[(cur_state,a)]\n",
    "        next_actions = list(next_action_prob.keys())\n",
    "        next_probs = list(next_action_prob.values())\n",
    "        next_state_idx = np.random.choice(len(next_actions), p=next_probs)\n",
    "        self.i = next_actions[next_state_idx][0]\n",
    "        self.j = next_actions[next_state_idx][1]\n",
    "        return self.rewards.get((self.i,self.j),0)\n",
    "\n",
    "    def is_terminal (self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def game_over(self):\n",
    "        return (self.i,self.j) in self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d089207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld  class\n",
    "class GridWorld():\n",
    "    \n",
    "    def __init__(self, rows, columns, start_position):\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        #self.all_states = [(i,j) for i in range(rows) for j in range(columns)]\n",
    "        self.i = start_position[0]\n",
    "        self.j = start_position[1]\n",
    "        \n",
    "    def set_rewards_actions(self, rewards, actions):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.all_states = set(self.actions.keys()) | set(self.rewards.keys())\n",
    "        print (self.all_states)\n",
    "    \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.i,self.j\n",
    "\n",
    "    def reset(self):\n",
    "        self.i = 2\n",
    "        self.j = 0\n",
    "        return self.current_state()\n",
    "    \n",
    "    def get_next_state(self, s, a):\n",
    "        i, j = s[0], s[1]\n",
    "        #print(f\"s : ({i},{j})\")\n",
    "        #print(a)\n",
    "        #print(self.actions[(i,j)])\n",
    "        if a in self.actions[(i,j)]:\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            elif a == 'R':\n",
    "                j += 1\n",
    "            elif a == 'L':\n",
    "                j -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "        #print(f\"s2 : ({i},{j})\")\n",
    "        return i,j\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action in self.actions[(self.i,self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j -= 1\n",
    "            elif action == 'L':\n",
    "                self.j += 1\n",
    "            else:\n",
    "                self.i -= 1\n",
    "        # should never happen\n",
    "        assert (self.current_state() in self.all_states)\n",
    " \n",
    "    def move(self, action):\n",
    "        if action in self.actions[(self.i,self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "            else:\n",
    "                self.i += 1\n",
    "        return self.rewards.get((self.i,self.j),0)\n",
    "\n",
    "    def is_terminal (self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def game_over(self):\n",
    "        return (self.i,self.j) in self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f02f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(V,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.columns):\n",
    "            v = V.get((i,j),0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print (\"\")\n",
    "\n",
    "def print_policy(P,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.columns):\n",
    "            a = P.get((i,j),' ')\n",
    "            print(\" %s |\" % a, end=\"\")\n",
    "        print (\"\")\n",
    "\n",
    "ACTION_SPACE = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def init_windy_grid_world_penalized(step_cost, start):\n",
    "    grid = WindyGridWorld(3,4,start)\n",
    "    rewards = {\n",
    "        (2,0):step_cost,\n",
    "        (1,0):step_cost,\n",
    "        (0,0):step_cost,\n",
    "        (2,1):step_cost,\n",
    "        (0,1):step_cost,\n",
    "        (2,2):step_cost,\n",
    "        (1,2):step_cost,\n",
    "        (0,2):step_cost,\n",
    "        (2,3):step_cost,\n",
    "        (0,3):1,\n",
    "        (1,3):-1\n",
    "    }\n",
    "    grid.set_rewards_actions(rewards, actions, probs)\n",
    "    return grid\n",
    "\n",
    "def max_dict(d):\n",
    "    '''\n",
    "    return the argmax and max value from a dictionnary\n",
    "    '''\n",
    "    max_val = max(d.values())\n",
    "    max_keys = [key for key, val in d.items() if val == max_val]\n",
    "    return np.random.choice(max_keys), max_val\n",
    "\n",
    "def epsilon_greedy(Q, state, eps=0.1):\n",
    "    '''\n",
    "    return an actions based on the epsilon greedy concept : exploration vs exploitation\n",
    "    random action if < epx else best action\n",
    "    '''\n",
    "    p = np.random.random()\n",
    "    if p < eps:\n",
    "        a = np.random.choice(ACTION_SPACE)\n",
    "    else:\n",
    "        a = Q[state]\n",
    "    return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6764aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "\n",
    "def main(step_cost, tot_iteration):\n",
    "    \n",
    "    gamma = 0.9    \n",
    "    ALPHA = 0.1\n",
    "        \n",
    "    # initialize G and returns\n",
    "    #g = init_grid_world_penalized(step_cost, (2,0))\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    g.set_rewards_actions(rewards=rewards, actions=actions)\n",
    "    \n",
    "    print(f\"rewards : \")\n",
    "    print_values(g.rewards,g)\n",
    "    \n",
    "    # use the given policy\n",
    "    \n",
    "    # initialize Q(s,a) and returns\n",
    "    V = {}\n",
    "    state_sample_count = {}\n",
    "    states = g.all_states\n",
    "    for s in states:\n",
    "        V[s]=0\n",
    "        state_sample_count[s]=0 \n",
    "      \n",
    "    # repeat until convergence\n",
    "    deltas = []\n",
    "    #loop on tot_iteration\n",
    "    for t in range(tot_iteration):\n",
    "        if t % 1000 == 0:\n",
    "            print(t)\n",
    "      \n",
    "        # play one game based on a random policy\n",
    "        biggest_change = 0\n",
    "        #print_policy(policy,g)\n",
    "        \n",
    "        cur_s = g.reset()\n",
    "        \n",
    "        #compute V based on rewards and next state\n",
    "        while g.game_over():\n",
    "            state_sample_count[cur_s] += 1 \n",
    "            a = epsilon_greedy(policy, cur_s)\n",
    "            \n",
    "            r = g.move(a)\n",
    "            next_s = g.current_state()              \n",
    "            \n",
    "            v_old = V[cur_s]\n",
    "            V[cur_s] = V[cur_s] + ALPHA * (r + gamma * V[next_s] - V[cur_s])\n",
    "\n",
    "            cur_s = next_s\n",
    "            \n",
    "            #update delta\n",
    "            biggest_change = max(biggest_change, np.abs(v_old - V[cur_s]))\n",
    "            deltas.append(biggest_change)\n",
    "        \n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"finale policy\")\n",
    "    print_policy(policy,g)\n",
    "    \n",
    "    # find V\n",
    "    print(\"final values:\")\n",
    "    print_values(V,g)\n",
    "    \n",
    "    \n",
    "    print(\"state_sample_count: \")\n",
    "    state_sample_count_arr = np.zeros((g.rows, g.columns))\n",
    "    for i in range(g.rows):\n",
    "        for j in range(g.columns):\n",
    "            if (i,j) in state_sample_count:\n",
    "                state_sample_count_arr[i,j] = state_sample_count[(i,j)]\n",
    "    df = pd.DataFrame(state_sample_count_arr)\n",
    "    print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331bf9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1), (1, 2), (0, 0), (1, 3), (2, 1), (2, 0), (2, 3), (2, 2), (1, 0), (0, 2), (0, 3)}\n",
      "rewards : \n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cur' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SANDRI~1\\AppData\\Local\\Temp/ipykernel_7720/4158025336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\SANDRI~1\\AppData\\Local\\Temp/ipykernel_7720/53388734.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(step_cost, tot_iteration)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mcur_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mstate_sample_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;31m#compute V based on rewards and next state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cur' is not defined"
     ]
    }
   ],
   "source": [
    "main(-0.05, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514294da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
