{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment description\n",
    "actions={\n",
    "    (2,0):['U','R'],\n",
    "    (1,0):['U','D'],\n",
    "    (0,0):['R','D'],\n",
    "    (2,1):['R','L'],\n",
    "    (0,1):['R','L'],\n",
    "    (2,2):['U','R','L'],\n",
    "    (1,2):['U','D','R'],\n",
    "    (0,2):['R','L','D'],\n",
    "    (2,3):['L'],\n",
    "}\n",
    "\n",
    "probs = {\n",
    "    ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "    ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "    ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "    ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "    ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "    ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "    ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "    ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "    ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "    ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "    ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "    ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "    ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "    ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld  class\n",
    "class GridWorld():\n",
    "    \n",
    "    def __init__(self, rows, columns, start_position):\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        #self.all_states = [(i,j) for i in range(rows) for j in range(columns)]\n",
    "        self.i = start_position[0]\n",
    "        self.j = start_position[1]\n",
    "        \n",
    "    def set_rewards_actions(self, rewards, actions, probs):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.probs = probs\n",
    "        self.all_states = set(self.actions.keys()) | set(self.rewards.keys())\n",
    "        #print (self.all_states)\n",
    "    \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    def current_state(self):\n",
    "        return i,j\n",
    "    \n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action in self.actions[(self.i,self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j -= 1\n",
    "            elif action == 'L':\n",
    "                self.j += 1\n",
    "            else:\n",
    "                self.i -= 1\n",
    "        # should never happen\n",
    "        assert (self.current_state() in self.all_states)\n",
    " \n",
    "    def move(self, action):\n",
    "        cur_state = (self.i, self.j)\n",
    "        a = action\n",
    "        next_action_prob = self.probs[(cur_state,a)]\n",
    "        next_actions = list(next_action_prob.keys)\n",
    "        next_probs = list(next_action_prob.values)\n",
    "        next_state = np.random.choice(next_actions, p=next_probs)\n",
    "        self.i = next_state[0]\n",
    "        self.j = next_state[1]\n",
    "        return self.rewards.get((self.i,self.j),0)\n",
    "\n",
    "    def is_terminal (self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def game_over():\n",
    "        return (self.i,self.j) in self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "\n",
    "def print_values(V,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.columns):\n",
    "            v = V.get((i,j),0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print (\"\")\n",
    "\n",
    "def print_policy(P,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.columns):\n",
    "            a = P.get((i,j),' ')\n",
    "            print(\" %s |\" % a, end=\"\")\n",
    "        print (\"\")\n",
    "\n",
    "ACTION_SPACE = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def init_grid_world_penalized(step_cost):\n",
    "    grid = GridWorld(3,4,(2,0))\n",
    "    rewards = {\n",
    "        (2,0):step_cost,\n",
    "        (1,0):step_cost,\n",
    "        (0,0):step_cost,\n",
    "        (2,1):step_cost,\n",
    "        (0,1):step_cost,\n",
    "        (2,2):step_cost,\n",
    "        (1,2):step_cost,\n",
    "        (0,2):step_cost,\n",
    "        (2,3):step_cost,\n",
    "        (0,3):1,\n",
    "        (1,3):-1\n",
    "    }\n",
    "    grid.set_rewards_actions(rewards, actions, probs)\n",
    "    return grid\n",
    "\n",
    "def init_transition_probs(grid, tr_probs, exp_rewards):\n",
    "    for (s,a),v in grid.probs.items():\n",
    "        for s2, p in v.items():\n",
    "            tr_probs[(s,a,s2)] = p\n",
    "            if s2 in grid.rewards:\n",
    "                exp_rewards[(s,a,s2)] = grid.rewards[s2]\n",
    "    return tr_probs, exp_rewards\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "def evaluate_deterministic_policy(grid, cur_policy, tr_probs, exp_rewards):\n",
    "    V = {}\n",
    "    for s in grid.all_states:\n",
    "        V[s]=0\n",
    "          \n",
    "    it = 0\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for s in grid.all_states:\n",
    "            if not grid.is_terminal(s):\n",
    "                old_v = V[s] \n",
    "                new_v = 0\n",
    "                for a in ACTION_SPACE:\n",
    "                    for s2 in grid.all_states:\n",
    "                        \n",
    "                        action_prob = 1 if cur_policy.get(s) == a else 0\n",
    "                        \n",
    "                        r = exp_rewards.get((s,a,s2),0)\n",
    "                        new_v += action_prob *tr_probs.get((s,a,s2),0)*(r + gamma *V[s2])\n",
    "                \n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "                \n",
    "        #print (\"iter: \", it, \"biggest_change: \", biggest_change)\n",
    "        #print_values(V,grid)\n",
    "        it += 1\n",
    "        \n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break   \n",
    "    return V\n",
    "    \n",
    "def play_game(step_cost):\n",
    "    \n",
    "    transition_probs = {}\n",
    "    expected_rewards = {}\n",
    "        \n",
    "    g = init_grid_world_penalized(step_cost)\n",
    "   \n",
    "    transition_probs, expected_rewards= init_transition_probs(g, transition_probs, expected_rewards) \n",
    "    #print (transition_probs)\n",
    "    #print (expected_rewards)\n",
    "    \n",
    "    # initialize main variable   \n",
    "    it_main = 0\n",
    "    V = {}\n",
    "    policy = {}\n",
    "    for s in g.all_states:\n",
    "        V[s]=0\n",
    "    \n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        #Evaluation the best value function\n",
    "        for s in g.all_states:\n",
    "            #print(f's:{s}')\n",
    "            if not g.is_terminal(s):\n",
    "                old_v = V[s]\n",
    "                new_v = float('-inf') \n",
    "                                \n",
    "                for a in ACTION_SPACE:    \n",
    "                    #print(a)\n",
    "                    v=0\n",
    "                    for s2 in g.all_states:          \n",
    "                        #print(s2)\n",
    "                        r = expected_rewards.get((s,a,s2),0)\n",
    "                        v += transition_probs.get((s,a,s2),0)*(r + gamma *V[s2])\n",
    "                \n",
    "                    if v > new_v:\n",
    "                        new_v = v\n",
    "                \n",
    "                V[s] = new_v \n",
    "                #print(f\"biggest_change : {biggest_change}, old_v : {old_v}, V[s] : {V[s]}\")\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "                   \n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "                                   \n",
    "        it_main +=1\n",
    "    \n",
    "    print (\"iter: \", it_main)\n",
    "    print_values(V,g)  \n",
    "\n",
    "    # loop once to find the associated best policy\n",
    "    for s in g.actions.keys():\n",
    "        if not g.is_terminal(s):\n",
    "            old_v = V[s]\n",
    "            new_v = float('-inf') \n",
    "                \n",
    "            for a in ACTION_SPACE:\n",
    "                v=0\n",
    "                for s2 in g.all_states:          \n",
    "                        \n",
    "                    r = expected_rewards.get((s,a,s2),0)\n",
    "                    v += transition_probs.get((s,a,s2),0)*(r + gamma *V[s2])\n",
    "                \n",
    "                if v > new_v:\n",
    "                    new_v = v\n",
    "                    policy[s] = a\n",
    "    \n",
    "    print_policy(policy,g)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  3\n",
      "---------------------------\n",
      " 0.05| 0.50| 1.00| 0.00|\n",
      "---------------------------\n",
      "-0.36| 0.00|-0.25| 0.00|\n",
      "---------------------------\n",
      "-0.72|-0.96|-0.62|-0.96|\n",
      "---------------------------\n",
      " R | R | R |   |\n",
      "---------------------------\n",
      " U |   | U |   |\n",
      "---------------------------\n",
      " U | R | U | L |\n"
     ]
    }
   ],
   "source": [
    "play_game(-0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
