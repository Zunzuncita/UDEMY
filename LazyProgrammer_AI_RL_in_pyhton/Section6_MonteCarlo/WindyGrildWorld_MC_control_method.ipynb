{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npolicy = {\\n    (2, 0): 'U',\\n    (1, 0): 'U',\\n    (0, 0): 'R',\\n    (0, 1): 'R',\\n    (0, 2): 'R',\\n    (1, 2): 'R',\\n    (2, 1): 'R',\\n    (2, 2): 'R',\\n    (2, 3): 'U'\\n  }\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment description\n",
    "rewards={(0,3):1,(1,3):-1}\n",
    "\n",
    "actions={\n",
    "    (2,0):['U','R'],\n",
    "    (1,0):['U','D'],\n",
    "    (0,0):['R','D'],\n",
    "    (2,1):['R','L'],\n",
    "    (0,1):['R','L'],\n",
    "    (2,2):['U','R','L'],\n",
    "    (1,2):['U','D','R'],\n",
    "    (0,2):['R','L','D'],\n",
    "    (2,3):['L'],\n",
    "}\n",
    "\n",
    "probs = {\n",
    "    ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "    ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "    ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "    ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "    ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "    ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "    ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "    ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "    ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "    ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "    ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "    ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "    ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "    ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "  }\n",
    "\n",
    "'''\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U'\n",
    "  }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld  class\n",
    "class GridWorld():\n",
    "    \n",
    "    def __init__(self, rows, columns, start_position):\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        #self.all_states = [(i,j) for i in range(rows) for j in range(columns)]\n",
    "        self.i = start_position[0]\n",
    "        self.j = start_position[1]\n",
    "        \n",
    "    def set_rewards_actions(self, rewards, actions, probs):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.probs = probs\n",
    "        self.all_states = set(self.actions.keys()) | set(self.rewards.keys())\n",
    "        #print (self.all_states)\n",
    "    \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.i,self.j\n",
    "    \n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action in self.actions[(self.i,self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j -= 1\n",
    "            elif action == 'L':\n",
    "                self.j += 1\n",
    "            else:\n",
    "                self.i -= 1\n",
    "        # should never happen\n",
    "        assert (self.current_state() in self.all_states)\n",
    " \n",
    "    def move(self, action):\n",
    "        cur_state = (self.i, self.j)\n",
    "        a = action\n",
    "        next_action_prob = self.probs[(cur_state,a)]\n",
    "        next_actions = list(next_action_prob.keys())\n",
    "        next_probs = list(next_action_prob.values())\n",
    "        next_state_idx = np.random.choice(len(next_actions), p=next_probs)\n",
    "        self.i = next_actions[next_state_idx][0]\n",
    "        self.j = next_actions[next_state_idx][1]\n",
    "        return self.rewards.get((self.i,self.j),0)\n",
    "\n",
    "    def is_terminal (self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def game_over(self):\n",
    "        return (self.i,self.j) in self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "\n",
    "def print_values(V,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.columns):\n",
    "            v = V.get((i,j),0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print (\"\")\n",
    "\n",
    "def print_policy(P,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.columns):\n",
    "            a = P.get((i,j),' ')\n",
    "            print(\" %s |\" % a, end=\"\")\n",
    "        print (\"\")\n",
    "\n",
    "ACTION_SPACE = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def init_grid_world_penalized(step_cost, start):\n",
    "    grid = GridWorld(3,4,start)\n",
    "    rewards = {\n",
    "        (2,0):step_cost,\n",
    "        (1,0):step_cost,\n",
    "        (0,0):step_cost,\n",
    "        (2,1):step_cost,\n",
    "        (0,1):step_cost,\n",
    "        (2,2):step_cost,\n",
    "        (1,2):step_cost,\n",
    "        (0,2):step_cost,\n",
    "        (2,3):step_cost,\n",
    "        (0,3):1,\n",
    "        (1,3):-1\n",
    "    }\n",
    "    grid.set_rewards_actions(rewards, actions, probs)\n",
    "    return grid\n",
    "\n",
    "def play_game(step_cost, policy, max_iteration):\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    # select a rando position to start the game\n",
    "    start_s = (np.random.randint(0,3),np.random.randint(0,4))\n",
    "    #print(f\"start position {start_s}\")\n",
    "    g = init_grid_world_penalized(step_cost, start_s)\n",
    "    #print_policy(policy, g)\n",
    "\n",
    "    cur_game_states_actions=[]\n",
    "    cur_game_rewards=[0]\n",
    "\n",
    "    #print(g.game_over())\n",
    "\n",
    "    #play one episode\n",
    "    while g.game_over() and t < max_iteration:\n",
    "        s = g.current_state()\n",
    "        #print(f\"current position {s}\")\n",
    "        a = policy[s]\n",
    "        cur_game_states_actions.append((s,a))\n",
    "        r = g.move(a)\n",
    "        #print(f\"reward {r}\")\n",
    "        cur_game_rewards.append(r)\n",
    "        t += 1\n",
    "        \n",
    "    s = g.current_state()\n",
    "    cur_game_states_actions.append((s,'')) \n",
    "    \n",
    "    return cur_game_states_actions, cur_game_rewards\n",
    "\n",
    "\n",
    "def main(step_cost, tot_iteration):\n",
    "    \n",
    "    gamma = 0.9    \n",
    "        \n",
    "    # initialize G and returns\n",
    "    g = init_grid_world_penalized(step_cost, (2,0))\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    for s in g.all_states:\n",
    "        for a in ACTION_SPACE:\n",
    "            Q[s,a]=[0,1]\n",
    "            returns[s,a]=[]\n",
    "    \n",
    "    it = 0\n",
    "    best_policy = {}\n",
    "    #loop on 100 iteration\n",
    "    for t in range(tot_iteration):\n",
    "    \n",
    "        #initialize a random policy\n",
    "        policy = {}\n",
    "        for state, v in actions.items():\n",
    "            policy[state] = np.random.choice(v)\n",
    "        #print_policy(policy,g)\n",
    "    \n",
    "        # play one game based on a random policy\n",
    "        cur_game_states_actions, cur_game_rewards = play_game(step_cost, policy, 20)\n",
    "    \n",
    "        #compute G and V based on results\n",
    "        G = 0\n",
    "        \n",
    "        game_length = len(cur_game_states_actions)\n",
    "        for i in range(game_length-1,-1,-1): \n",
    "            G = cur_game_rewards[i]+gamma*G\n",
    "            last_state, last_action = cur_game_states_actions[i-1]\n",
    "            if not g.is_terminal(last_state):\n",
    "                if (last_state, last_action) not in cur_game_states_actions.pop():\n",
    "                    returns[last_state, last_action].append(G)\n",
    "                    #print(f\"{last_state}, {last_action} : {returns[last_state, last_action][-1]}, {Q[last_state, last_action]}\")\n",
    "                    Q[last_state, last_action] = [(returns[last_state, last_action][-1]+ (Q[last_state, last_action][1]*Q[last_state, last_action][0]))/(Q[last_state, last_action][1] + 1),Q[last_state, last_action][1] + 1]\n",
    "                    q_old = float('-inf')\n",
    "                    v = g.actions[last_state]\n",
    "                    #print(v)\n",
    "                    for a in v:\n",
    "                        q = Q[last_state, a][0]\n",
    "                        if q > q_old:\n",
    "                            best_policy[last_state]=a\n",
    "                            q_old = q\n",
    "\n",
    "        it += 1\n",
    "    print (\"iter: \", it)\n",
    "    print_policy(best_policy,g)\n",
    "        \n",
    "    print(\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  10000\n",
      "---------------------------\n",
      " R | R | R |   |\n",
      "---------------------------\n",
      " U |   | D |   |\n",
      "---------------------------\n",
      " U | L | R | L |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(-0.05, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
